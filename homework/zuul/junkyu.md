
[Zuul2](http://techblog.netflix.com/2016/09/zuul-2-netflix-journey-to-asynchronous.html)
==

최근에 넷플릭스는 자신들의  클라우드 게이트웨이인 Zuul의 아키텍쳐에 변화를 줬다고 한다.

### (배경지식)
> 여기서 게이트웨이란 다른 서버들의 중개자로 동작하는 서버로,. 주로 HTTP 트래픽을 다른 프로토콜로 변환하기 위해 사용된다. 게이트웨이는 항상 자신이 리소스를 갖고 있는 진짜 서버인 것철덤 요청을 다루기에, 클라이언트는 게이트웨이와 통신하고 있는 것을 알아채지 못한다. 게이트웨이는 서로 다른 프로토콜과 애플리케이션 간의 HTTP 인터페이스라고 볼 수 있으며, 리소스와 애플리케이션을 연결하는 역할을 한다.

아무튼, Zuul2 또한 Zuul1이 했던 것을 똑같이 수행하는데, 예를 들면, 넷플릭스 서버 인프라의 첫번째 문으로 수행하는 것이나, 전세계의 넷플릭스 유저로부터의 트래픽을 처리하는 것과 같은 것이다.

Zuul2는 또한 리퀘스트를 라우팅하고 개발자의 테스팅과 디버깅을 지원하고, 전체적인 서비스 헬스에 대한 깊은 인사이트를 제공하고, 공격으로 부터 넷플릭스를 보호한다. 그리고 AWS region에 문제가 있을 때 트래픽을 다른 클라우드 region으로 옮긴다. 

Zuul과 Zuul2의 가장 큰 아키텍쳐상의 차이점은 Zuul2는 Netty를 이용한 비동기 논블락킹 프레임워크에서 실행된다는 것이다. 지난 몇달동안의 프로덕션 레벨에서의 운영 후에, 최초의 이점은 Zuul2가 디바이스들과 웹 브라우저들이 Netflix scale내에서 Netflix가 Persistent Connection을 맺도록 도와줄 수 있는  capability를 제공한다는 것이었다. 이렇게 persistent connection을 클라우드 인프라에서 맺음으로써 많은 기능과 혁신을 만들어 낼 수 있었고, 전체적인 디바이스의 리퀘스트를 줄이며, 디바이스의 퍼포먼스를 향상시키고 고객의 경험을 더 잘 디버깅할 수 있게 되었다. 또한 Zuul2는 아마 latency, throughput, cost의 관점에서 레질리언시한 이익과 성능 향상을 제공할 것이다.  그러나 이제부터 나올 post에서 볼 수 있듯이,  넷플릭스의 기대는 결과와 좀 달랐다.

Zuul1은 서블릿 프레임워크이다. 이는 블락킹이고 멀티스레드기반인데 이는 그들이 리퀘스트를 하나의 커넥션당 하나의 스레드를 사용한다는 것을 의미한다. I/O 작업은 I/O를 실행시키기 위해서 스레드풀에서 워커 스레드를 고르면서 수행된다. 또한, 리퀘스트 스레드는 워커 스레드가 일을 끝낼때까지 블락된다. 워커 스레드는 자신의 작업이 끝날때 리퀘스트 스레드에게 통지를 해준다. 이것은 현재의 concurrent connection을 100% 다루는 멀티코어 AWS 인스턴스상에서 잘 작동한다.  그러나 backend latency 증가나 에러로 인한 디바이스의 재요청과 같은 상황이 악화된 경우에는 액티브 커넥션과 스레드의 수가 증가한다. 그래서 이것이 일어나면, 각 노드들에 문제가 생기게 되고 백업 스레드들이 서버 부하를 주며 cluster를 극복하는 death spiral로 빠지게 된다. 이런 리스크들을 없애기 위해서 넷플릭스는 Throttling 메카니즘과 라이브러리들(e.g. Hystrix)을 개발했고 이를 통해 넷플릭스의 블락킹 시스템이 위와 같은 상황동안 안정적으로 유지되도록 돕고자 하였다.

(throttling 이란 무엇일까?)

이와 다르게, 비동기 시스템은 일반적으로 하나의 CPU마다 하나의 스레드를 통해 모든 리퀘스트와 리스폰스를 처리하면서 동작한다. 리퀘스트와 리스폰스의 라이프사이클은 이벤트와 콜백을 통해서 처리된다. 각 리퀘스트마다의 스레드가 없기 때문에, 커넥션의 비용은 매우 싸다. 이것은 file descripter의 비용이고 listener의 비용이다. (반면 블락킹 모델에서의 커넥션은 스레드이고 헤비한 메모리와 시스템 오버헤드를 가진다.) 몇몇의 효율을 얻는데, 이는 좀더 CPU 레벨의 캐시의 사용하도록 만들며, 더 적은 context switche를 필요로 하기에, 같은 CPU에서 데이터가 머무르는 것을 통해 효율을 얻는다.
Backend latency와 retry storms(사용자와 장치가 어떠한 문제로 인해 재요청을 하는 것)의 결과는 또한 덜 stressful하다. 왜냐하면 커넥션들과 큐에서의 증가한 이벤트들은 스레드가 늘어나는 것보다 훨씬 덜 비싸기 때문이다.

(리눅스의 file discripter에 대해서 나중에 찾아보기)

Async 시스템의 장점은 좋아보인다. 하지만 이런 이점들 위에 운영의 비용이 있다. Blocking 시스템은 이해하고 디버그하기 쉽다. 하나의 스레드가 항상 하나의 작업만 하기 때문에, 스레드의 스택이 리퀘스트나 늘어진 테스크의 진행상황의 정확한 스냅샷이 된다. 그리고 하나의 스레드 덤프는 뒤따르는 락들에 의해 멀티 스레드로 확장하는 리퀘스트를 따르는 것으로 보여질 수 있다. (thread dump란 무엇인가.?) 발생한 예외는 단지 스택에서 팝된다. 'catch-call' 익셉션 핸들러는 의도적으로 잡지못한 모든 것을 제거해준다.

이와는 정반대로, Aync는 callback based이고 event loop에 의해 주도된다. 이벤트 루프의 스택 추적은 리퀘스트를 따라가도록 시도할때 무의미하다. 이벤트와 콜백이 진행되는 동안 리퀘스트를 따라가는것은 어렵고, 이것을 디버깅할 수 잇는 툴또한 이 분야에선 거의 없다고 봐야한다. Edge cases, unhandled exceptions, 그리고 부정확하게 처리된 상태 변화는 댕글링 리소스를 생상하는데 그 예로는 ByteBuf leaks, fijle dscriptor leaks, lost responses, 등등이다. 이런 타입의 이슈들은 디버깅하기 매우 어렵다고 증명이 되었는데, 왜냐하면 이는 어느 이벤트가 적절하게 처리되지 않았고 혹은 적절히 제거됬는지를 알기가 정말 어렵기 때문이다.

Building Non-blocking Zuul
넷플릭스의 인프라내에서 Zuul2를 만드는 것은 기대했던 것보다 더 도전적이었다. 넷플리스 에코시스템내에서의 많은 서비스들은 블락킹이 되도록 만들어졌었다. 넷플릭스의 코어 네트워크 라이브러리들 또한 블락킹 아키텍쳐에 대한 기대와 함께 만들어 졌었는데, 많은 라이브러리들이 리퀘스트에 대한 컨텍스트를 만들고 저장하기 위해 스레드의 지역변수에 의존한다. 결과적으로, Zuul2를 만드는 많은 복잡도가 스레드 지역변수가 사용되어지는 dark corner를 떼어내는 것이었다. 다른 어려움들은 Blocking 네트워크 로직을 Non-blocking 네트워크 코드로 바꾸는 것과 라이브러리들 내부의 깊숙한 곳에 있는 blocking 코드를 찾는 것과 resource leak을 수정하는 것, 그리고 코어 인프라를 비동기적으로 실행시키도록 변환하는 것과 연관이 있었다. Blocking 네트워크 로직을 어싱크로 변환하는 단 하나의 전략은 없다. 블락킹 네트워크 로직들은 개별로 분석되어야만 하고 리팩토링되어야만 한다. 이와 같은 것은 넷플릭스의 코어 라이브러리에 똑같이 적용되는데, 이 라이브러리들에서 몇몇 코드들은 수정되었고 몇몇은 forked되어야만 했고 또 비동기와 함께 작업하도록 리팩토링되었다. 오픈소스 프로젝트의 Reactive-Audit은 매우 유용했는데, 넷플릭스의 서버들이 어디의 코드가 블락되고 library가 블락을 거는지에 대한 케이스를 찾는 것을 실험하는데 매우 유용했다.

넷플릭스는 Zuul2를 만드는데 흥미로운 어프로치를 했다. Blocking 시스템은 코드를 비동기적으로 실행시킬 수 있기 때문에, 넷플릭스는 Zuul Filters, 그리고 비동기적으로 실행시키기 위한 코드를 체이닝하는 filter를 처음으로 바꿈으로써 시작했다. Zuul Filters는 특정한 로직을 포함하는데, 이 로직은 넷플릭스가 게이트웨이 기능을 하기위해 만든 것이다.(라우팅, 로깅, 리버스 프락싱, ddos prevention, etc) 넷플릭스는 Zuul의 코어, Zuul Filter 클래스, 그리고 코드를 비동기적으로 실행하도록 허락하는 RxJava를 사용하는 Zuul Filter를 리팩토링 했다. 그래서 지금까지 두 종류의 필터가 같이 쓰인다. 첫번째가 I/O 작업을 위해 사용되는 async, 두번째가 I/O 작업을 필요로 하지않는 로지컬한 작업을 실행하는 async filter. Async Zuul Filter는 blocking/non-blocking 시스템에서 완전히 같은 필터 로직을 실행하도록 허락한다. 이것이 넷플릭스에게 하나의 필터와 함께 작업하는 능력을 주었는데, 이는 넷플릭스가 싱글 코드베이스의 Netty-based 아키텍쳐를 개발하는 도중에 파트너들을 위해 게이트웨이 기능을 개발할 수 있기 위해서였다.?? Async Zuul Filter와 함께, Zuul2를 개발하는 것은 "단지" 나머지의 Zuul 인프라스트럭쳐를 비동기적으로, 논블락킹으로 만드는 것의 문제였다. 이와 같이 Zuul Filters도 단지 두개의 인프라스트럭쳐에 drop into할 수 잇게 되었다??

Result of Zuul 2 in Production
넷플릭스의 게이트웨이와 함께 비동기 아키텍쳐의 이익에 대해 매우 많은 가정이 존재한다. 엄청난 규모의 순서에 관련된 몇몇 생각들은 효율성에서 증가했는데, 이는 컨텍스트 스위칭의 감소와 CPU 캐시를 사용하는 것에 대한 효율, 그리고 처음에는 전혀 효율을 얻지 못할 것이라는 다른 예상들 때문이었다. 변화와 개발의 노력에 대한 복잡도의 관점에서는 또 의견이 다양하다.

그래서 넷플릭스는 이런 아키텍쳐의 변화를 함으로써 무엇을 얻었나? 이것이 가치가 있었을까? 이 주제는 정말 핫한 논쟁거리다.  클라우드 게이트웨이 팀은 넷플릭스에서 async-based 서비스를 만들고 테스트하는데 선구자로서의 노력을 다했다. async를 사용한 microservice가 넷플릭스에서 어떻게 작동하는지에 대해, Zuul이 어떻게 이익을 얻기위한 이상적인 서비스인지를 이해하는 데 매우 흥미로운 것이 많다. 

넷플릭스가 async와 non-blocking으로 마이그레이팅하면서 매우 큰 효율성의 이익을 보지는 못했던 반면, 넷플릭스는 connection scaling의 목표를 성취했다. 


